{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chatbot summarization with postgress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN9HDBEiqzCv",
        "outputId": "0f1fd811-9336-4503-94e2-d29201a4fbc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/415.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/415.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m409.6/415.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.1/415.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.7/198.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --quiet -U langchain_groq langchain_core langgraph langgraph-checkpoint-postgres psycopg psycopg-pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GhiXf2tEdpwA"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "apikey = userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TYtAWV6Ld1ZZ"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(model= 'llama-3.3-70b-versatile', temperature = 0, api_key=apikey)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bdibyXpSd5qs"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "DB_URI = userdata.get('DB_URI')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KjqlmL75egS9"
      },
      "outputs": [],
      "source": [
        "from psycopg_pool import ConnectionPool\n",
        "from langgraph.checkpoint.postgres import PostgresSaver\n",
        "\n",
        "# Connection pool for efficient database access\n",
        "connection_kwargs = {\"autocommit\": True, \"prepare_threshold\": 0}\n",
        "\n",
        "# Create a persistent connection pool\n",
        "pool = ConnectionPool(conninfo=DB_URI, max_size=20, kwargs=connection_kwargs)\n",
        "\n",
        "# Initialize PostgresSaver checkpointer\n",
        "checkpointer = PostgresSaver(pool)\n",
        "checkpointer.setup()  # Ensure database tables are set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Dln5tMU7ehXY"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "\n",
        "from langgraph.graph import END\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "class State(MessagesState):\n",
        "    summary: str\n",
        "\n",
        "# Define the logic to call the model\n",
        "def call_model(state: State) -> State:\n",
        "\n",
        "    # Get summary if it exists\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "    print(f\"Using summary: {summary}\")\n",
        "\n",
        "    # If there is summary, then we add it\n",
        "    if summary:\n",
        "\n",
        "        # Add summary to system message\n",
        "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
        "\n",
        "        # Append summary to any newer messages\n",
        "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "\n",
        "    else:\n",
        "        messages = state[\"messages\"]\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def summarize_conversation(state: State) -> State:\n",
        "    print(f\"Messages before summarizing: {len(state['messages'])}\")\n",
        "    # First, we get any existing summary\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "    print(f\"Existing summary: {summary}\")\n",
        "\n",
        "    # Create our summarization prompt\n",
        "    if summary:\n",
        "\n",
        "        # A summary already exists\n",
        "        summary_message = (\n",
        "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
        "            \"Extend the summary by taking into account the new messages above:\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        summary_message = \"Create a summary of the conversation above:\"\n",
        "\n",
        "\n",
        "    # Add prompt to our history\n",
        "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
        "    response = llm.invoke(messages)\n",
        "    # Summarization logic\n",
        "    print(f\"New summary: {response.content}\")\n",
        "\n",
        "    # Delete all but the 2 most recent messages\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "\n",
        "    print(f\"Messages after truncation: {len(delete_messages)}\")\n",
        "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
        "\n",
        "# Determine whether to end or summarize the conversation\n",
        "def should_continue(state: State) -> State:\n",
        "\n",
        "    \"\"\"Return the next node to execute.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    print(f\"Message count: {len(messages)}\")\n",
        "    # If there are more than six messages, then we summarize the conversation\n",
        "    if len(messages) > 6:\n",
        "        return \"summarize_conversation\"\n",
        "\n",
        "    # Otherwise we can just end\n",
        "    return END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zJbWbQ6NepmV"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "# Redefine workflow\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(\"conversation\", call_model)\n",
        "workflow.add_node(summarize_conversation)\n",
        "\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
        "workflow.add_edge(\"summarize_conversation\", END)\n",
        "\n",
        "# Compile the workflow with PostgreSQL checkpointer\n",
        "graph = workflow.compile(checkpointer=checkpointer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiGuAX7MfSMz",
        "outputId": "4333041f-aa8d-41c1-cc77-f713d85fef57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using summary: \n",
            "Message count: 2\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Zeeshan! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Zeeshan\", additional_kwargs={}, response_metadata={}, id='dbe3ffe1-05fe-452f-bd8c-1a2d7a1e07ec'), AIMessage(content=\"Hello Zeeshan! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 42, 'total_tokens': 70, 'completion_time': 0.101818182, 'prompt_time': 0.004619813, 'queue_time': 0.28877038200000005, 'total_time': 0.106437995}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_76dc6cf67d', 'finish_reason': 'stop', 'logprobs': None}, id='run-07ecc975-b88b-4357-9136-bb0a7d7505e3-0', usage_metadata={'input_tokens': 42, 'output_tokens': 28, 'total_tokens': 70})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1eff99ec-e274-658a-8001-c2934318d38c'}}, metadata={'step': 1, 'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content=\"Hello Zeeshan! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'logprobs': None, 'model_name': 'llama-3.3-70b-versatile', 'token_usage': {'queue_time': 0.28877038200000005, 'total_time': 0.106437995, 'prompt_time': 0.004619813, 'total_tokens': 70, 'prompt_tokens': 42, 'completion_time': 0.101818182, 'completion_tokens': 28}, 'finish_reason': 'stop', 'system_fingerprint': 'fp_76dc6cf67d'}, id='run-07ecc975-b88b-4357-9136-bb0a7d7505e3-0', usage_metadata={'input_tokens': 42, 'output_tokens': 28, 'total_tokens': 70})}}, 'parents': {}, 'thread_id': '1'}, created_at='2025-03-05T08:49:53.060359+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1eff99ec-dccd-6139-8000-75efe4439058'}}, tasks=())"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Configuration for thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Start a conversation\n",
        "input_message = HumanMessage(content=\"hi! I'm Zeeshan\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "# Check the persisted state\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irTkhWuOf1xS",
        "outputId": "40b147cc-48c7-493f-97c7-b795486b6fa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using summary: \n",
            "Message count: 4\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Coding can be a lot of fun. What kind of coding do you enjoy? Are you into web development, mobile app development, or something else? Do you have a favorite programming language?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Zeeshan\", additional_kwargs={}, response_metadata={}, id='dbe3ffe1-05fe-452f-bd8c-1a2d7a1e07ec'), AIMessage(content=\"Hello Zeeshan! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 42, 'total_tokens': 70, 'completion_time': 0.101818182, 'prompt_time': 0.004619813, 'queue_time': 0.28877038200000005, 'total_time': 0.106437995}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_76dc6cf67d', 'finish_reason': 'stop', 'logprobs': None}, id='run-07ecc975-b88b-4357-9136-bb0a7d7505e3-0', usage_metadata={'input_tokens': 42, 'output_tokens': 28, 'total_tokens': 70}), HumanMessage(content='I like coding.', additional_kwargs={}, response_metadata={}, id='fac4f585-3d63-4e36-8ad1-2d01a80c5517'), AIMessage(content='Coding can be a lot of fun. What kind of coding do you enjoy? Are you into web development, mobile app development, or something else? Do you have a favorite programming language?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 83, 'total_tokens': 122, 'completion_time': 0.145329509, 'prompt_time': 0.006160269, 'queue_time': 0.230033416, 'total_time': 0.151489778}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ca0059abb', 'finish_reason': 'stop', 'logprobs': None}, id='run-c90451a2-c420-49cf-a870-dcc65137c191-0', usage_metadata={'input_tokens': 83, 'output_tokens': 39, 'total_tokens': 122})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1eff99ed-c5d5-6882-8004-e695fc3d3e50'}}, metadata={'step': 4, 'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content='Coding can be a lot of fun. What kind of coding do you enjoy? Are you into web development, mobile app development, or something else? Do you have a favorite programming language?', additional_kwargs={}, response_metadata={'logprobs': None, 'model_name': 'llama-3.3-70b-versatile', 'token_usage': {'queue_time': 0.230033416, 'total_time': 0.151489778, 'prompt_time': 0.006160269, 'total_tokens': 122, 'prompt_tokens': 83, 'completion_time': 0.145329509, 'completion_tokens': 39}, 'finish_reason': 'stop', 'system_fingerprint': 'fp_2ca0059abb'}, id='run-c90451a2-c420-49cf-a870-dcc65137c191-0', usage_metadata={'input_tokens': 83, 'output_tokens': 39, 'total_tokens': 122})}}, 'parents': {}, 'thread_id': '1'}, created_at='2025-03-05T08:50:16.902840+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1eff99ed-c097-6164-8003-beb42d13e030'}}, tasks=())"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Configuration for thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Start a conversation\n",
        "input_message = HumanMessage(content=\"I like coding.\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "# Check the persisted state\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBtzNvUpf8hB",
        "outputId": "b8e3787f-066d-43ad-b1d0-c68234fa8347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using summary: \n",
            "Message count: 6\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Zeeshan, and your hobby is coding.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Zeeshan\", additional_kwargs={}, response_metadata={}, id='dbe3ffe1-05fe-452f-bd8c-1a2d7a1e07ec'), AIMessage(content=\"Hello Zeeshan! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 42, 'total_tokens': 70, 'completion_time': 0.101818182, 'prompt_time': 0.004619813, 'queue_time': 0.28877038200000005, 'total_time': 0.106437995}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_76dc6cf67d', 'finish_reason': 'stop', 'logprobs': None}, id='run-07ecc975-b88b-4357-9136-bb0a7d7505e3-0', usage_metadata={'input_tokens': 42, 'output_tokens': 28, 'total_tokens': 70}), HumanMessage(content='I like coding.', additional_kwargs={}, response_metadata={}, id='fac4f585-3d63-4e36-8ad1-2d01a80c5517'), AIMessage(content='Coding can be a lot of fun. What kind of coding do you enjoy? Are you into web development, mobile app development, or something else? Do you have a favorite programming language?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 83, 'total_tokens': 122, 'completion_time': 0.145329509, 'prompt_time': 0.006160269, 'queue_time': 0.230033416, 'total_time': 0.151489778}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ca0059abb', 'finish_reason': 'stop', 'logprobs': None}, id='run-c90451a2-c420-49cf-a870-dcc65137c191-0', usage_metadata={'input_tokens': 83, 'output_tokens': 39, 'total_tokens': 122}), HumanMessage(content=\"What's my name and what is my hobby?\", additional_kwargs={}, response_metadata={}, id='f8763ac9-92b1-4fb3-9369-7c5456f95dba'), AIMessage(content='Your name is Zeeshan, and your hobby is coding.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 141, 'total_tokens': 155, 'completion_time': 0.050909091, 'prompt_time': 0.008534019, 'queue_time': 0.24629509, 'total_time': 0.05944311}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_e669a124b2', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2c78117-b53e-46fd-9fd8-889cedded46a-0', usage_metadata={'input_tokens': 141, 'output_tokens': 14, 'total_tokens': 155})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1eff99ee-7f1e-6687-8007-7b488d00ebc2'}}, metadata={'step': 7, 'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content='Your name is Zeeshan, and your hobby is coding.', additional_kwargs={}, response_metadata={'logprobs': None, 'model_name': 'llama-3.3-70b-versatile', 'token_usage': {'queue_time': 0.24629509, 'total_time': 0.05944311, 'prompt_time': 0.008534019, 'total_tokens': 155, 'prompt_tokens': 141, 'completion_time': 0.050909091, 'completion_tokens': 14}, 'finish_reason': 'stop', 'system_fingerprint': 'fp_e669a124b2'}, id='run-a2c78117-b53e-46fd-9fd8-889cedded46a-0', usage_metadata={'input_tokens': 141, 'output_tokens': 14, 'total_tokens': 155})}}, 'parents': {}, 'thread_id': '1'}, created_at='2025-03-05T08:50:36.331346+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1eff99ee-7a78-63b2-8006-f3e118c6b7f8'}}, tasks=())"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Configuration for thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Start a conversation\n",
        "input_message = HumanMessage(content=\"What's my name and what is my hobby?\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "# Check the persisted state\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
